{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eef5acf-b81b-4f0a-8f3c-18a55f36d226",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Synthesis Example using Conditional Tabular GAN (CTGAN)\n",
    "Unlike the example in `R`, this example uses a library in `Python` that implements a few different types of `Generative Adversarial Networks`, or GANs, specifically Conditional Tabular GAN and Conditional Probabilistic Auto-Regressive (CPAR) GANs.  This example will focus on CTGAN which can be used to synthesize data in general and can be used as a way to synthesize records in a manner similar to SMOTE.\n",
    "\n",
    "## Differences in approaches\n",
    "While the CART-based approach synthesizes a single variable at time, some GAN-based methods synthesize vectors (the entire record) at one time. CTGAN is one of those methods.  However, while it might seem like this would speed things up substantially, the computational complexity of the underlying architecture may not always play out that way, especially if you are doing this on CPU only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfe939e7-76eb-4991-860f-a537ef1c2b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stdid              int32\n",
       "altsch              int8\n",
       "urbanicity        object\n",
       "cohort             int16\n",
       "male             float64\n",
       "race              object\n",
       "frleverhs        float64\n",
       "swdeverhs           int8\n",
       "eleverhs            int8\n",
       "tageverhs           int8\n",
       "alteverhs           int8\n",
       "mthss6           float64\n",
       "rlass6           float64\n",
       "mthss8           float64\n",
       "rlass8           float64\n",
       "pctabshs         float64\n",
       "pctexcusedhs     float64\n",
       "hsgpa            float64\n",
       "acteng11         float64\n",
       "actmth11         float64\n",
       "actrla11         float64\n",
       "actcmp11         float64\n",
       "evercollrdyhs       int8\n",
       "evercarrdyhs        int8\n",
       "aptakenever         int8\n",
       "lastobsyr          int16\n",
       "transfer            int8\n",
       "dropout             int8\n",
       "stillenrolled       int8\n",
       "gradontime          int8\n",
       "gradcohort       float64\n",
       "diploma             int8\n",
       "yr1psenrany      float64\n",
       "yr1psenr2yr      float64\n",
       "yr1psenr4yr      float64\n",
       "yr2psenrany      float64\n",
       "schid             object\n",
       "dtype: object"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loads library to allow setting the pseudorandom number generator seed\n",
    "import random\n",
    "\n",
    "# Sets the pseudorandom number generator seed\n",
    "random.seed(7779311)\n",
    "\n",
    "# Load libary to read the data\n",
    "import pandas as pd \n",
    "\n",
    "# Columns from the Faketucky file that we will keep and their new names\n",
    "cols = { 'sid': 'stdid', 'first_dist_code': 'distid', 'first_hs_code': 'schcd', \n",
    "        'first_hs_alt': 'altsch', 'first_hs_urbanicity': 'urbanicity', \n",
    "        'chrt_ninth': 'cohort', 'male': 'male', 'race_ethnicity': 'race', \n",
    "        'frpl_ever_in_hs': 'frleverhs', 'sped_ever_in_hs': 'swdeverhs', \n",
    "        'lep_ever_in_hs': 'eleverhs', 'gifted_ever_in_hs': 'tageverhs', \n",
    "        'ever_alt_sch_in_hs': 'alteverhs', 'scale_score_6_math': 'mthss6', \n",
    "        'scale_score_6_read': 'rlass6', 'scale_score_8_math': 'mthss8', \n",
    "        'scale_score_8_read': 'rlass8', 'pct_absent_in_hs': 'pctabshs', \n",
    "        'pct_excused_in_hs': 'pctexcusedhs', 'avg_gpa_hs': 'hsgpa', \n",
    "        'scale_score_11_eng': 'acteng11', 'scale_score_11_math': 'actmth11', \n",
    "        'scale_score_11_read': 'actrla11', 'scale_score_11_comp': 'actcmp11', \n",
    "        'collegeready_ever_in_hs': 'evercollrdyhs', 'careerready_ever_in_hs': 'evercarrdyhs', \n",
    "        'ap_ever_take_class': 'aptakenever', 'last_acadyr_observed': 'lastobsyr', \n",
    "        'transferout': 'transfer', 'dropout': 'dropout', 'still_enrolled': 'stillenrolled', \n",
    "        'ontime_grad': 'gradontime', 'chrt_grad': 'gradcohort', 'hs_diploma': 'diploma', \n",
    "        'enroll_yr1_any': 'yr1psenrany', 'enroll_yr1_2yr': 'yr1psenr2yr', \n",
    "        'enroll_yr1_4yr': 'yr1psenr4yr', 'enroll_yr2_any': 'yr2psenrany' }\n",
    "\n",
    "# Load the data and rename the columns to shorter names\n",
    "df1 = pd.read_stata('https://github.com/OpenSDP/faketucky/raw/master/faketucky.dta', columns = cols.keys()).rename(columns = cols)\n",
    "\n",
    "# Create the combined school/district ID\n",
    "df1['schid'] = df1['distid'].astype(str) + df1['schcd'].astype(str)\n",
    "\n",
    "# Get a sample of school IDs\n",
    "schids = df1['schid'].drop_duplicates().sample(n = 60, random_state = 7779311)\n",
    "\n",
    "# Inner Join to select the subset of cases with the sampled school IDs\n",
    "df = df1.merge(schids, how = 'inner', on = 'schid')\n",
    "\n",
    "# Remove the school and district codes that created schid\n",
    "df.drop(columns = ['distid', 'schcd'], inplace = True)\n",
    "\n",
    "# Make sure a couple columns/variables are correctly typed for the synthesis software\n",
    "df['urbanicity'] = df['urbanicity'].astype(str)\n",
    "df['schid'] = df['schid'].astype(str)\n",
    "\n",
    "# Show some of the data\n",
    "df.head(20)\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5ebf41d-4332-4416-8259-7786312c11e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CudaDeviceProperties(name='Quadro RTX 5000', major=7, minor=5, total_memory=16117MB, multi_processor_count=48)\n",
      "GPU Available = True\n",
      "# of GPUS = 1\n"
     ]
    }
   ],
   "source": [
    "# Load the torch library\n",
    "import torch \n",
    "\n",
    "# Check to see if a GPU is available so you use a GPU later \n",
    "gpu = torch.cuda.is_available()\n",
    "\n",
    "# Check how many GPU are available\n",
    "ngpus = torch.cuda.device_count()\n",
    "\n",
    "# For GPU setups\n",
    "if ngpus >= 1 and ngpus is not None:\n",
    "    # Get the device properties for each GPU available\n",
    "    for i in range(ngpus):\n",
    "        print(torch.cuda.get_device_properties(i))\n",
    "\n",
    "# Print the result so you can make sure the GPU is ID'd in case something goes wrong with CUDA\n",
    "print('GPU Available = {0}\\n# of GPUS = {1}'.format(gpu, ngpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afed9937-dec6-4920-a1c3-cd5a72dcd3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have multiple GPUs here is where you can set which GPU to use based on the index\n",
    "# starting from 0\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d05b30ba-8179-4e2c-baf1-0a8b65a247fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/billy/anaconda3/lib/python3.9/site-packages/rdt/transformers/numerical.py:100: UserWarning: No rounding scheme detected for column 'pctabshs'. Data will not be rounded.\n",
      "  warnings.warn(\n",
      "/home/billy/anaconda3/lib/python3.9/site-packages/rdt/transformers/numerical.py:100: UserWarning: No rounding scheme detected for column 'pctexcusedhs'. Data will not be rounded.\n",
      "  warnings.warn(\n",
      "/home/billy/anaconda3/lib/python3.9/site-packages/rdt/transformers/numerical.py:100: UserWarning: No rounding scheme detected for column 'hsgpa'. Data will not be rounded.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G:  9.1588,Loss D: -15.8004\n",
      "1 epochs took 0.5858280380566915 minutes to complete\n"
     ]
    }
   ],
   "source": [
    "# Import CTGAN from the Synthetic Data Vault library\n",
    "from sdv.tabular import CTGAN\n",
    "\n",
    "# Use this to set the number of epochs (passes over your data) for training\n",
    "eps = 1\n",
    "\n",
    "# You can import this module if you want to see the timing\n",
    "import time\n",
    "\n",
    "# This is where you can define/tune your specific architecture for your CTGAN\n",
    "# See: https://sdv.dev/SDV/api_reference/tabular/api/sdv.tabular.ctgan.CTGAN.html#sdv.tabular.ctgan.CTGAN \n",
    "# for a description of all the parameters that are available.\n",
    "# cuda = whether to use a GPU or not\n",
    "# discriminator_steps = Typically this is 5 or 10, it is how many times the discriminator is updated before updating the generator\n",
    "# pac = # of samples grouped together when training the discriminator\n",
    "# batch_size = How many observations from your dataset should be processed at a time\n",
    "# verbose = whether to print updates during training indicating some type of progress\n",
    "# embedding_dim = # of nodes to use for the embedding used for conditioning\n",
    "# generator_dim = Specify the number of layers/nodes (#, #) each # is the number of nodes in the layer\n",
    "# discriminator_dim = Specify the number of layers/nodes (#, #) each # is the number of nodes in the layer\n",
    "# epochs = The # of passes over your dataset to train the model\n",
    "# You define the architecture without any data.  Once defined you pass data to it using the fit() method.\n",
    "ctmod = CTGAN(cuda = gpu,\n",
    "              discriminator_steps = 5,\n",
    "              pac = 50,\n",
    "              batch_size = 4000,\n",
    "              verbose = True,\n",
    "              embedding_dim = 256,\n",
    "              generator_dim = (512, 512, 256, 256, 128, 128),\n",
    "              discriminator_dim = (1024, 512, 256, 128), \n",
    "              epochs = eps)\n",
    "\n",
    "# Leave this uncommented if you want to time the training\n",
    "start = time.time()\n",
    "\n",
    "# This trains the model\n",
    "ctmod.fit(df)\n",
    "\n",
    "# Leave this uncommented if you \n",
    "end = time.time()\n",
    "\n",
    "# Leave this uncommented if the timing stuff is uncommented to see how long it took\n",
    "print('{} epochs took {} minutes to complete'.format(eps, (end - start)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea39ffef-7ddf-43fb-ab3a-daf34d3fa088",
   "metadata": {},
   "source": [
    "# Synthesizing Data\n",
    "Once your model has been trained to your liking, which will definitely take more than a single epoch, you can begin synthesizing data from it.  The code block below shows not only how to synthesize a whole sample, but also how you can use CTGAN to synthesize data for specific groups, like SMOTE but probably with greater fidelity.  \n",
    "\n",
    "## Notes on Synthesizing Specific Groups\n",
    "Like any data synthesis, you need to be careful and aware of what your protected data looks like with regards to representation of your population of interest.  Data synthesis isn't magic, nor can it infer what our intentions are.  The process is building a mathematical model to best describe the distribution of the data we provide so it can sample from it.  So, if you believe the data you have for some of your smaller groups of interest is a good approximation for what the data for that group would look like in repeated samples you should be fine synthesizing from that group.  If, however, you believe that the data you have for the group you are interested in is not a good representation of that group, I would recommend **against** synthesizing data for the group.  That said, also keep in mind that the more data you use to train the model, the better the results will be.  Even if your goal is only to synthesize data for a single group, include data from all of the other groups you have available since there is additional information in the relationships between the variables that can improve the quality of the synthetic records for the group you are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "140e4a33-5bc6-43d8-9fd7-a0efd7116e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: 100%|█████████████████████████████████████████████| 400/400 [00:10<00:00, 39.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# To generate synthetic data use the sample method\n",
    "synthtest = ctmod.sample(330)\n",
    "\n",
    "# Then you can save it to a file\n",
    "synthtest.to_csv('synthestCTGAN.csv')\n",
    "\n",
    "# And delete the object to reduce memory consumption\n",
    "del synthtest\n",
    "\n",
    "# To sample a specific group, define the conditions\n",
    "from sdv.sampling import Condition\n",
    "\n",
    "# Define the groups you need synthetic records for\n",
    "remoteaa = Condition(num_rows = 100, column_values = {'urbanicity': 'Rural: Remote', 'race': 'African-American'})\n",
    "remotelx = Condition(num_rows = 100, column_values = {'urbanicity': 'Rural: Remote', 'race': 'Hispanic'})\n",
    "fringeaa = Condition(num_rows = 100, column_values = {'urbanicity': 'Rural: Fringe', 'race': 'African-American'})\n",
    "fringelx = Condition(num_rows = 100, column_values = {'urbanicity': 'Rural: Fringe', 'race': 'Hispanic'})\n",
    "\n",
    "# Then use the sample_conditions() method to generate samples with the characteristics you need\n",
    "condtest = ctmod.sample_conditions(conditions = [remoteaa, remotelx, fringeaa, fringelx])\n",
    "\n",
    "# And you can save those data to a file as well:\n",
    "condtest.to_csv('condtestCTGAN.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
